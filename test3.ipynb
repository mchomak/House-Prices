{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_squared_error\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numerical data columns use mean score\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# for categorial data columns fill in empty values \"missing\"\n",
    "# OneHotEncoder split categorial data columns into columns with Boolean values (0, 1)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse = False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "# PCA use for reduction data columns and get major information\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Choose the number of components based on the explained variance threshold\n",
    "n_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "pca = PCA(n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_features(df):\n",
    "    df_out = df.copy()\n",
    "    df_out['PropertyAge'] = df_out['YrSold'] - df_out['YearBuilt']\n",
    "    df_out['TotalSF'] = df_out['TotalBsmtSF'] + df_out['1stFlrSF'] + df_out['2ndFlrSF']\n",
    "    df_out['TotalBath'] = df_out['FullBath'] + 0.5 * df_out['HalfBath'] + df_out['BsmtFullBath'] + 0.5 * df['BsmtHalfBath']\n",
    "    df_out['HasRemodeled'] = (df_out['YearRemodAdd'] != df_out['YearBuilt']).astype(object)\n",
    "    df_out['Has2ndFloor'] = (df_out['2ndFlrSF'] > 0).astype(object)\n",
    "    df_out['HasGarage'] = (df_out['GarageArea'] > 0).astype(object)\n",
    "    df_out['YrSold_cat'] = df_out['YrSold'].astype(object)\n",
    "    df_out['MoSold_cat'] = df_out['MoSold'].astype(object)\n",
    "    df_out['YearBuilt_cat'] = df_out['YearBuilt'].astype(object)\n",
    "    df_out['MSSubClass_cat'] = df_out['MSSubClass'].astype(object)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "feature_engineering_transformer = FunctionTransformer(custom_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols_categorical = pd.Index(['HasRemodeled', 'Has2ndFloor', 'HasGarage'])\n",
    "new_cols_numeric = pd.Index(['PropertyAge', 'TotalSF', 'TotalBath', 'YrSold_cat', 'MoSold_cat', 'YearBuilt_cat', 'MSSubClass_cat'])\n",
    "\n",
    "# Update categorical and numerical columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.append(new_cols_categorical)\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.append(new_cols_numeric)\n",
    "\n",
    "# Remove target variable from numerical columns\n",
    "numerical_columns = numerical_columns.drop('SalePrice')\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "# remainder = 'passthrough' columns were are not specified will be passed\n",
    "# remainder = 'drop' columns were are not specified will be delete\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ],remainder = 'passthrough')\n",
    "\n",
    "\n",
    "# Create a pipeline with the preprocessor\n",
    "pipeline_fe = Pipeline(steps=[\n",
    "    ('fe', feature_engineering_transformer),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', pca)])\n",
    "\n",
    "# Apply the pipeline to your dataset\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "y = np.log(df['SalePrice'])\n",
    "X_preprocessed_fe = pipeline_fe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fe, X_test_fe, y_train_fe, y_test_fe = train_test_split(X_preprocessed_fe, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    'XGBoost': XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'max_depth': [3, 6, 10],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-fold cross-validation\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Train and tune the models\n",
    "grids = {}\n",
    "for model_name, model in models.items():\n",
    "    #print(f'Training and tuning {model_name}...')\n",
    "    grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "    grids[model_name].fit(X_train_fe, y_train_fe)\n",
    "    best_params = grids[model_name].best_params_\n",
    "    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n",
    "    \n",
    "    print(f'Best parameters for {model_name}: {best_params}')\n",
    "    print(f'Best RMSE for {model_name}: {best_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
